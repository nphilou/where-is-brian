{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/philippe/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/philippe/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english = pandas.read_csv(\"train_cap2018.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27310,)\n(27310, 6)\n"
     ]
    }
   ],
   "source": [
    "target = english['level1']\n",
    "target.head()\n",
    "print(target.shape)\n",
    "\n",
    "# data\n",
    "data = english.loc[:, \"fulltext\":\"punct\"]\n",
    "print(data.shape)\n",
    "\n",
    "# words_data : \n",
    "text = english.loc[:, \"fulltext\"]\n",
    "words_data = text.map(lambda t: word_tokenize(t))\n",
    "\n",
    "\n",
    "# print(data.loc[3][\"fulltext\"][0])\n",
    "# sentence = data[:3][\"fulltext\"][0]\n",
    "# sentence = data.as_matrix()[27309, 0]\n",
    "# print(sentence)\n",
    "# words = word_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sixteen',\n 'headphones',\n '.',\n 'also',\n 'there',\n 'are',\n 'thirty',\n 'pens',\n 'and',\n 'fifteen',\n 'pencils',\n '.',\n 'also',\n 'there',\n 'are',\n 'twelve',\n 'desks',\n ',',\n 'fourteen',\n 'chairs',\n 'and',\n 'one',\n 'table',\n '.',\n 'bye']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_data[27304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', \"'s\", 'Jean-Marc', '.', 'I', \"'m\", 'a', 'management', 'controller', '.', 'I', 'work', 'in', 'an', 'office', '.', 'My', 'favorite', 'day', 'in', 'Sunday', '.', 'I', 'get', 'up', 'at', 'eight', \"o'clock\", 'in', 'the', 'morning', 'on', 'Sunday', ',', 'have', 'a', 'shower', ',', 'brush', 'my', 'teeth', 'and', 'go', 'running', '.', 'I', 'have', 'lunch', 'at', 'twelve', \"o'clock\", 'in', 'the', 'afternoon', '.', 'I', 'go', 'swimming', 'in', 'the', 'evening', ',', 'I', 'meet', 'friends', '.', 'We', 'eat', 'in', 'a', 'restaurant', 'then', 'go', 'to', 'the', 'movies', 'and', 'watch', 'a', 'movie', '.', 'I', 'go', 'to', 'bed', 'at', 'eleven', \"o'clock\", '.']\n52\n91\n0.571428571429\n0.623853211009\n0.69387755102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0.579767\n1        0.609137\n2        0.693878\n3        0.647959\n4        0.570732\n5        0.619289\n6        0.673759\n7        0.504902\n8        0.597285\n9        0.586592\n10       0.756757\n11       0.503597\n12       0.619718\n13       0.518072\n14       0.623711\n15       0.666667\n16       0.592814\n17       0.632353\n18       0.559748\n19       0.638743\n20       0.615385\n21       0.734043\n22       0.656863\n23       0.600877\n24       0.632124\n25       0.542105\n26       0.586022\n27       0.592814\n28       0.607595\n29       0.617347\n           ...   \n27280    0.326531\n27281    0.950000\n27282    0.608108\n27283    0.627907\n27284    0.701754\n27285    0.937500\n27286    0.717391\n27287    0.916667\n27288    0.754717\n27289    0.784314\n27290    0.742857\n27291    0.820000\n27292    0.840909\n27293    0.730769\n27294    0.740741\n27295    0.818182\n27296    0.562500\n27297    0.441176\n27298    0.784314\n27299    0.784615\n27300    0.714286\n27301    0.781250\n27302    0.710526\n27303    0.616438\n27304    0.760000\n27305    0.571429\n27306    0.752475\n27307    0.789474\n27308    0.911765\n27309    0.794872\nName: fulltext, Length: 27310, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tagged = nltk.pos_tag(words_data[27304])\n",
    "\n",
    "print words_data[27305]\n",
    "\n",
    "print len(set(words_data[27305]))\n",
    "\n",
    "print len(words_data[27305])\n",
    "print (len(set(words_data[27305]))/len(words_data[27305]))\n",
    "print (len(set(words_data[3000]))/len(words_data[3000]))\n",
    "print (len(set(words_data[2]))/len(words_data[2]))\n",
    "\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "\n",
    "ld = words_data.map(lambda t: lexical_diversity(t))\n",
    "ld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "y = english.groupby('level1').sentences.count()\n",
    "plt.bar(y.keys(), y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n     multi_class='ovr', penalty='l2', random_state=0, tol=0.0001,\n     verbose=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = svm.SVC(gamma=0.001, C=100.)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(random_state=0) \n",
    "data_cleaned = data.T.dropna().T\n",
    "\n",
    "clf.fit(data_cleaned, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentences  words  letters.all  syllables  punct\n0         16    233         1116        363     32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['B1'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned.head()\n",
    "\n",
    "print(data_cleaned[:1])\n",
    "clf.predict(data_cleaned[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 48 ms, total: 1min 29s\nWall time: 1min 50s\n[0.5610247  0.56232839 0.57817649 0.5521978  0.54542125]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "%time listerr = cross_val_score(clf, data_cleaned, target, cv=5)\n",
    "print listerr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "data_standard = StandardScaler().fit_transform(data_cleaned)\n",
    "corr = np.corrcoef(data_standard.T)\n",
    "corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.66837766, 0.64175596, 0.64342692, 0.73276522],\n       [0.66837766, 1.        , 0.98341281, 0.98538917, 0.71997432],\n       [0.64175596, 0.98341281, 1.        , 0.99595057, 0.70399435],\n       [0.64342692, 0.98538917, 0.99595057, 1.        , 0.71004385],\n       [0.73276522, 0.71997432, 0.70399435, 0.71004385, 1.        ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sea\n",
    "sea.heatmap(corr)\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
