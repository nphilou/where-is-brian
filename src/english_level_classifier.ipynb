{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet d Analyse et Fouille de donnees \"My taylor is rich\"\n",
    "======================================================================================================\n",
    "\n",
    "\n",
    "Nguyen Philippe  \n",
    "Nourry Charles\n",
    "\n",
    "## Solving a classification problem using scikit-learn and keras\n",
    "\n",
    "The major parts of our projects are:\n",
    "\n",
    "* Text loading, cleaning and lemmatization\n",
    "* Word grouping for each level\n",
    "* Study of word distributions for each class\n",
    "* Training data with a neural network\n",
    "* Study of the results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries needed for this project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nourr\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nourr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\nourr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nourr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "import seaborn as sea\n",
    "\n",
    "import keras\n",
    "import scipy\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "We read the data, then we separate it in two parts, a part with the text and an other with the statistics.  \n",
    "In our project we will focus mainly on the text part, which is much more significant for predicting the level of English.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = pandas.read_csv(\"train_cap2018.csv\") # Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27310,)\n",
      "(27310, 58)\n"
     ]
    }
   ],
   "source": [
    "target = english['level1']\n",
    "target.head()\n",
    "print(target.shape)\n",
    "\n",
    "# data :\n",
    "data = english.loc[:, \"sentences\":\"Yule's K\"]\n",
    "print(data.shape)\n",
    "\n",
    "# words_data : \n",
    "text = english.loc[:, \"fulltext\"]\n",
    "words_data = text.map(lambda t: word_tokenize(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'my', 'name', \"'s\", 'Jean-Marc', '.', 'I', \"'m\", 'a', 'management', 'controller', '.', 'I', 'work', 'in', 'an', 'office', '.', 'My', 'favorite', 'day', 'in', 'Sunday', '.', 'I', 'get', 'up', 'at', 'eight', \"o'clock\", 'in', 'the', 'morning', 'on', 'Sunday', ',', 'have', 'a', 'shower', ',', 'brush', 'my', 'teeth', 'and', 'go', 'running', '.', 'I', 'have', 'lunch', 'at', 'twelve', \"o'clock\", 'in', 'the', 'afternoon', '.', 'I', 'go', 'swimming', 'in', 'the', 'evening', ',', 'I', 'meet', 'friends', '.', 'We', 'eat', 'in', 'a', 'restaurant', 'then', 'go', 'to', 'the', 'movies', 'and', 'watch', 'a', 'movie', '.', 'I', 'go', 'to', 'bed', 'at', 'eleven', \"o'clock\", '.']\n",
      "52\n",
      "91\n",
      "0.5714285714285714\n",
      "0.6238532110091743\n",
      "0.6938775510204082\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(words_data[27304])\n",
    "\n",
    "print(words_data[27305])\n",
    "\n",
    "print(len(set(words_data[27305])))\n",
    "\n",
    "print(len(words_data[27305]))\n",
    "print(len(set(words_data[27305])) / len(words_data[27305]))\n",
    "print(len(set(words_data[3000])) / len(words_data[3000]))\n",
    "print(len(set(words_data[2])) / len(words_data[2]))\n",
    "\n",
    "\n",
    "def lexical_diversity(text):\n",
    "    return len(set(text)) / len(text)\n",
    "\n",
    "\n",
    "ld = words_data.map(lambda t: lexical_diversity(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping of data\n",
    "\n",
    "Data is grouped according to English levels. The words used are very significant to the level of English, a beginner will not use the same words as a bilingual.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>[The, Eiffel, Tower, is, a, symbol, of, domina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>[Since, our, meeting, I, 've, heard, your, req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>[At, the, beginning, of, the, metting, the, CE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>[Hello, my, friend, ,, I, would, like, to, tel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>[Dear, friends, ,, excuse, me, but, I, could, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>[Hello, My, name, 's, David, ., I, 'm, from, F...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words\n",
       "level                                                   \n",
       "C2     [The, Eiffel, Tower, is, a, symbol, of, domina...\n",
       "C1     [Since, our, meeting, I, 've, heard, your, req...\n",
       "B2     [At, the, beginning, of, the, metting, the, CE...\n",
       "B1     [Hello, my, friend, ,, I, would, like, to, tel...\n",
       "A2     [Dear, friends, ,, excuse, me, but, I, could, ...\n",
       "A1     [Hello, My, name, 's, David, ., I, 'm, from, F..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word lists for each instance\n",
    "pdf = pandas.DataFrame({'level': target, 'words': words_data})\n",
    "\n",
    "# words grouped according to the level\n",
    "grouped = pdf.groupby(['level'], sort=False).sum()\n",
    "grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization of words\n",
    "\n",
    "it is essential to lemmatize the words, in this way we will be able to recognize all the similar words, which have been used in the plural or conjugated in the original data.  \n",
    "We apply this process twice, once on the data that we grouped by class and another time directly on the instances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>level</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C2</th>\n",
       "      <td>{hospital, developped, pay, boy, suspiciously,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C1</th>\n",
       "      <td>{developped, pay, klee, charged, am, represent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B2</th>\n",
       "      <td>{duality, developped, pay, house.Indeed, centr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B1</th>\n",
       "      <td>{developped, pay, similar-, Initial, way.Subje...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2</th>\n",
       "      <td>{developped, gymnasium, apetizers, transportat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1</th>\n",
       "      <td>{18Hair, pay, difficult.the, menu.Starter, cri...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   words\n",
       "level                                                   \n",
       "C2     {hospital, developped, pay, boy, suspiciously,...\n",
       "C1     {developped, pay, klee, charged, am, represent...\n",
       "B2     {duality, developped, pay, house.Indeed, centr...\n",
       "B1     {developped, pay, similar-, Initial, way.Subje...\n",
       "A2     {developped, gymnasium, apetizers, transportat...\n",
       "A1     {18Hair, pay, difficult.the, menu.Starter, cri..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lemmatization of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "grouped_in_set = grouped.applymap(lambda l: set(map(lemmatizer.lemmatize, l)))\n",
    "grouped_in_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_words_count = grouped_in_set.applymap(lambda s: len({w for w in s if w[0].islower()}))\n",
    "diff_words = grouped_in_set.applymap(lambda s: {w for w in s if w[0].islower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization of words for every instance\n",
    "words_lem = words_data.apply(lambda l: set(map(lemmatizer.lemmatize, l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1759"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_words_count[\"words\"][\"C2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.695946</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.695946</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.770270</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.652542</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.728814</td>\n",
       "      <td>0.737288</td>\n",
       "      <td>0.838983</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.706767</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.706767</td>\n",
       "      <td>0.729323</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.832000</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.756522</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.730435</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         A1        A2        B1        B2        C1        C2 level\n",
       "0  0.675676  0.695946  0.702703  0.695946  0.675676  0.770270    C2\n",
       "1  0.652542  0.745763  0.745763  0.728814  0.737288  0.838983    C2\n",
       "2  0.684211  0.706767  0.736842  0.706767  0.729323  0.842105    C2\n",
       "3  0.720000  0.760000  0.768000  0.760000  0.760000  0.832000    C2\n",
       "4  0.713043  0.756522  0.782609  0.791304  0.730435  0.869565    C2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# probabilities of each instance of belonging to each class\n",
    "dist_C2 = pandas.DataFrame({'C2': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"C2\"])/len(t)),\n",
    "                            'C1': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"C1\"])/len(t)),\n",
    "                            'B2': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"B2\"])/len(t)),\n",
    "                            'B1': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"B1\"])/len(t)),\n",
    "                            'A2': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"A2\"])/len(t)),\n",
    "                            'A1': words_lem.map(lambda t: len(t & diff_words[\"words\"][\"A1\"])/len(t)),\n",
    "                            'level': target\n",
    "                            })\n",
    "\n",
    "dist_C2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram of class distribution\n",
    "This figure shows that the data are not uniformly distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2769cbf2940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# histogram of class distribution\n",
    "fig = plt.figure()\n",
    "y = english.groupby('level1').sentences.count()\n",
    "plt.bar(y.keys(), y.values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of a neural network on the data\n",
    "\n",
    "We use keras to build and train our neural network.  \n",
    "After processing the data in such a way that we can use them in our neural network. We let the network train for 50 steps.  \n",
    "This allows us to obtain a validation of accurancy of 86%.  \n",
    "The neural network is trained on 70% of the data. We will then test our model on the remaining 30% to verify that our model is good\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19117/19117 [==============================] - 3s 170us/step - loss: 1.2798 - acc: 0.4305\n",
      "Epoch 2/50\n",
      "19117/19117 [==============================] - 3s 132us/step - loss: 1.1052 - acc: 0.5091\n",
      "Epoch 3/50\n",
      "19117/19117 [==============================] - 3s 136us/step - loss: 0.9877 - acc: 0.5711\n",
      "Epoch 4/50\n",
      "19117/19117 [==============================] - 3s 141us/step - loss: 0.8837 - acc: 0.6257\n",
      "Epoch 5/50\n",
      "19117/19117 [==============================] - 2s 121us/step - loss: 0.8012 - acc: 0.6668\n",
      "Epoch 6/50\n",
      "19117/19117 [==============================] - 2s 116us/step - loss: 0.7344 - acc: 0.7021\n",
      "Epoch 7/50\n",
      "19117/19117 [==============================] - 2s 109us/step - loss: 0.6799 - acc: 0.7252\n",
      "Epoch 8/50\n",
      "19117/19117 [==============================] - 2s 107us/step - loss: 0.6459 - acc: 0.7414\n",
      "Epoch 9/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.6017 - acc: 0.7643\n",
      "Epoch 10/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.5770 - acc: 0.7731\n",
      "Epoch 11/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.5521 - acc: 0.7873\n",
      "Epoch 12/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.5361 - acc: 0.7884\n",
      "Epoch 13/50\n",
      "19117/19117 [==============================] - 2s 106us/step - loss: 0.5135 - acc: 0.7999\n",
      "Epoch 14/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.5018 - acc: 0.8021\n",
      "Epoch 15/50\n",
      "19117/19117 [==============================] - 2s 119us/step - loss: 0.4850 - acc: 0.8100\n",
      "Epoch 16/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.4846 - acc: 0.8128\n",
      "Epoch 17/50\n",
      "19117/19117 [==============================] - 2s 112us/step - loss: 0.4683 - acc: 0.8170\n",
      "Epoch 18/50\n",
      "19117/19117 [==============================] - 2s 110us/step - loss: 0.4605 - acc: 0.8192\n",
      "Epoch 19/50\n",
      "19117/19117 [==============================] - 2s 111us/step - loss: 0.4529 - acc: 0.8224\n",
      "Epoch 20/50\n",
      "19117/19117 [==============================] - 2s 113us/step - loss: 0.4532 - acc: 0.8243\n",
      "Epoch 21/50\n",
      "19117/19117 [==============================] - 2s 114us/step - loss: 0.4332 - acc: 0.8260\n",
      "Epoch 22/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.4407 - acc: 0.8274\n",
      "Epoch 23/50\n",
      "19117/19117 [==============================] - 2s 104us/step - loss: 0.4315 - acc: 0.8304\n",
      "Epoch 24/50\n",
      "19117/19117 [==============================] - 2s 126us/step - loss: 0.4324 - acc: 0.8291\n",
      "Epoch 25/50\n",
      "19117/19117 [==============================] - 2s 120us/step - loss: 0.4255 - acc: 0.8320\n",
      "Epoch 26/50\n",
      "19117/19117 [==============================] - 2s 110us/step - loss: 0.4076 - acc: 0.8360\n",
      "Epoch 27/50\n",
      "19117/19117 [==============================] - 2s 109us/step - loss: 0.4140 - acc: 0.8353\n",
      "Epoch 28/50\n",
      "19117/19117 [==============================] - 2s 128us/step - loss: 0.4044 - acc: 0.8398\n",
      "Epoch 29/50\n",
      "19117/19117 [==============================] - 2s 119us/step - loss: 0.4026 - acc: 0.8397\n",
      "Epoch 30/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.3931 - acc: 0.8442\n",
      "Epoch 31/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.4006 - acc: 0.8408\n",
      "Epoch 32/50\n",
      "19117/19117 [==============================] - 2s 119us/step - loss: 0.3868 - acc: 0.8452\n",
      "Epoch 33/50\n",
      "19117/19117 [==============================] - 2s 118us/step - loss: 0.3858 - acc: 0.8471\n",
      "Epoch 34/50\n",
      "19117/19117 [==============================] - 2s 126us/step - loss: 0.3803 - acc: 0.8468\n",
      "Epoch 35/50\n",
      "19117/19117 [==============================] - 2s 130us/step - loss: 0.3760 - acc: 0.8508\n",
      "Epoch 36/50\n",
      "19117/19117 [==============================] - 2s 116us/step - loss: 0.3865 - acc: 0.8470\n",
      "Epoch 37/50\n",
      "19117/19117 [==============================] - 2s 114us/step - loss: 0.3839 - acc: 0.8473\n",
      "Epoch 38/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.3806 - acc: 0.8501\n",
      "Epoch 39/50\n",
      "19117/19117 [==============================] - 2s 114us/step - loss: 0.3692 - acc: 0.8499\n",
      "Epoch 40/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.3679 - acc: 0.8514\n",
      "Epoch 41/50\n",
      "19117/19117 [==============================] - 2s 122us/step - loss: 0.3631 - acc: 0.8539\n",
      "Epoch 42/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.3645 - acc: 0.8524\n",
      "Epoch 43/50\n",
      "19117/19117 [==============================] - 2s 115us/step - loss: 0.3729 - acc: 0.8519\n",
      "Epoch 44/50\n",
      "19117/19117 [==============================] - 2s 116us/step - loss: 0.3569 - acc: 0.8546\n",
      "Epoch 45/50\n",
      "19117/19117 [==============================] - 2s 121us/step - loss: 0.3607 - acc: 0.8563\n",
      "Epoch 46/50\n",
      "19117/19117 [==============================] - 2s 116us/step - loss: 0.3604 - acc: 0.8542\n",
      "Epoch 47/50\n",
      "19117/19117 [==============================] - 2s 116us/step - loss: 0.3561 - acc: 0.8569\n",
      "Epoch 48/50\n",
      "19117/19117 [==============================] - 2s 123us/step - loss: 0.3598 - acc: 0.8555\n",
      "Epoch 49/50\n",
      "19117/19117 [==============================] - 2s 117us/step - loss: 0.3567 - acc: 0.8531\n",
      "Epoch 50/50\n",
      "19117/19117 [==============================] - 2s 117us/step - loss: 0.3542 - acc: 0.8579\n"
     ]
    }
   ],
   "source": [
    "# training of a neural network on the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.models\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "data_cleaned = english.loc[:, \"sentences\":\"Yule's K\"].T.dropna().T\n",
    "dist = dist_C2.loc[:, \"A1\":\"C2\"]\n",
    "\n",
    "cols = dist.shape[1] + data_cleaned.shape[1]\n",
    "\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(Dense(512, input_shape=(6,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(dist_C2.loc[:, \"level\"])\n",
    "\n",
    "encode_Y = encoder.transform(dist_C2.loc[:, \"level\"])\n",
    "Y = np_utils.to_categorical(encode_Y)\n",
    "\n",
    "# with tf.device('/gpu:0'):\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(dist,Y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "#model.fit(dist,Y, epochs=50, batch_size=32) \n",
    "model.fit(X_train,Y_train, epochs=50, batch_size=32)\n",
    "\n",
    "# + données du csv\n",
    "\n",
    "frames = [dist, data_cleaned]\n",
    "\n",
    "X = pandas.concat(frames, axis=1)\n",
    "\n",
    "# model.fit(X, Y, epochs=40, batch_size=32) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8193/8193 [==============================] - 0s 45us/step\n",
      "Score: 0.30\n",
      "Validation Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print(\"Score: %.2f\" % (score))\n",
    "print(\"Validation Accuracy: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "The confusion matrix is calculated from the data whose level of English is known.  \n",
    "We compare the classes of the data with the results predicted by our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7929,   18,    0,    0,    0,    0],\n",
       "       [1061, 4248,   77,    0,    0,    0],\n",
       "       [ 256,  235, 3259,    0,    1,    0],\n",
       "       [  67,   99,  307, 1171,    2,    0],\n",
       "       [  11,   13,   43,    0,  282,    0],\n",
       "       [   0,    2,   35,    1,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confusion matrix\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#y_true = dist_C2.loc[:, \"level\"]\n",
    "#y_predict = model.predict(dist_C2.loc[:, \"A1\":\"C2\"], batch_size=32)\n",
    "\n",
    "y_t = Y_train\n",
    "y_true = []\n",
    "y_predict = model.predict(X_train)\n",
    "for i in range (0,np.shape(y_t)[0]):\n",
    "    pos, = np.where(y_t[i] == 1)\n",
    "    y_true.append(level[pos[0]])\n",
    "\n",
    "y_pred = []\n",
    "level = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "for i in range (0,np.shape(y_predict)[0]):\n",
    "    max = np.max(y_predict[i])\n",
    "    pos, = np.where(y_predict[i] == max)\n",
    "    y_pred.append(level[pos[random.randint(0,len(pos)-1)]])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"])\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
